{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "import re\n",
    "from urllib.parse import urlparse, urlsplit\n",
    "import os\n",
    "import random\n",
    "import time \n",
    "from scipy.stats import uniform, levy\n",
    "import numpy as np\n",
    "import calendar\n",
    "import numpy as np \n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows\n"
     ]
    }
   ],
   "source": [
    "sys = platform.system()\n",
    "print(sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Igor\\Desktop\\scraping_medium\\\n",
      "C:\\Users\\Igor\\Desktop\\scraping_medium\\results_medium_standard\\\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "if sys == 'Windows':\n",
    "    current_folder =  str(os.getcwd()+'\\\\')\n",
    "    path_results = current_folder + 'results_medium_standard\\\\'\n",
    "else:\n",
    "    current_folder =  str(os.getcwd()+'/')\n",
    "    path_results = current_folder + 'results_medium_standard/'\n",
    "print(current_folder)\n",
    "print(path_results)\n",
    "print(' ')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_to_search = 'medium.com'\n",
    "loc = '+loc%3Abr'\n",
    "cc = '+cc%3Apt-BR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords_list = ['animais', 'artes', 'atualidades', 'bebes', 'beleza', 'ciencia', 'cinema',\\\n",
    "#                  'cultura',\\\n",
    "#                  'decoracao', 'design', 'economia',\\\n",
    "#                  'educacao', 'emprego', 'entretenimento', 'espiritualidade', 'esportes', 'estilo+de+vida',\\\n",
    "#                  'familia', 'financas', 'fitness', 'futebol', 'gastronomia', 'humor',\\\n",
    "#                  'infantil', 'internet', 'jornal', 'jornalismo', 'literatura', 'meio+ambiente', 'moda',\\\n",
    "#                  'musica', 'natureza', 'negocios',  'noticias', 'politica', 'saude', 'tecnologia', 'televisao', \\\n",
    "#                  'tv', 'variedades', 'viagens']\n",
    "\n",
    "keywords_list = ['animais']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that generates a random number to be used as the sleep time. This\n",
    "# uses a mixture of normal and levy distribution in order to bypass the server firewalls.\n",
    "def random_sleep_time():\n",
    "    dt = 0\n",
    "    while dt > 30 or dt < 2 :\n",
    "        dt = 1.5*uniform.rvs(size=1)[0]*levy.rvs(size=1)[0]\n",
    "    return dt    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that corrects the end day of a given month. For example, 28/02 or 29/02?\n",
    "def end_day_corrector(day, month, year):\n",
    "\n",
    "    try:\n",
    "       \n",
    "        date_list = [str(month), str(day), str(year)]\n",
    "        final_date = '-'.join(date_list)        \n",
    "        \n",
    "        d = datetime.strptime(final_date, '%m-%d-%Y')\n",
    "        \n",
    "        \n",
    "    except ValueError:\n",
    "        #print('wrong final day of the month!!!')\n",
    "        #day = day -1\n",
    "        final_date = end_day_corrector(day-1, month, year)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return final_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unix_date_day(day, month, year):\n",
    "    \n",
    "    input_date = end_day_corrector(day, month, year)\n",
    "    \n",
    "    d = datetime.strptime(input_date, '%m-%d-%Y')\n",
    "    \n",
    "    unixtime = calendar.timegm(d.utctimetuple())\n",
    "    unixtime_day = unixtime/60/60/24\n",
    "    \n",
    "    return unixtime_day\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2007   1   [13514.0, 13544.0]\n",
      "1   2007   2   [13545.0, 13572.0]\n",
      "2   2007   3   [13573.0, 13603.0]\n",
      "3   2007   4   [13604.0, 13633.0]\n",
      "4   2007   5   [13634.0, 13664.0]\n",
      "5   2007   6   [13665.0, 13694.0]\n",
      "6   2007   7   [13695.0, 13725.0]\n",
      "7   2007   8   [13726.0, 13756.0]\n",
      "8   2007   9   [13757.0, 13786.0]\n",
      "9   2007   10   [13787.0, 13817.0]\n",
      "10   2007   11   [13818.0, 13847.0]\n",
      "11   2007   12   [13848.0, 13878.0]\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "unix_time_dict = {}\n",
    "interval_complete_list = []\n",
    "\n",
    "month_year_list = []\n",
    "k= 0\n",
    "for year in range(2007, 2008):\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        \n",
    "  \n",
    "        start_date = unix_date_day(1, month, year)\n",
    "\n",
    "        end_date = unix_date_day(31, month, year)\n",
    "\n",
    "\n",
    "\n",
    "        interval_date_list = [start_date, end_date]\n",
    "\n",
    "        print(k, ' ',  year, ' ', month, ' ', interval_date_list)\n",
    "        k += 1\n",
    "       \n",
    "        interval_complete_list.append(interval_date_list)\n",
    "        month_year_list.append(str(month) + '_' + str(year))\n",
    "        \n",
    "        \n",
    "        if year == 2018 and month >= 5:\n",
    "            break\n",
    "    \n",
    "print(' ')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_start:  0\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(str(path_results + 'idx_medium_forward.dat')) == True:\n",
    "    f = open(path_results + 'idx_medium_forward.dat', 'r' )\n",
    "    start = f.readlines()[0]\n",
    "    idx_start = int(start)\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    idx_start = 0\n",
    "    \n",
    "print('idx_start: ', idx_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_list = range(2007, 2008, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.bing.com/search?q=animais+site%3Amedium.com+cc%3Apt-BR&qs=HS&sc=3-0&cvid=E2EAF9B741E34472B3DF8F49392E96AF&sp=1&first=9&FORM=PERE1\n",
      "animais   2007 step:  9\n",
      "sleep time:  0.9380593254279319\n",
      " \n",
      " \n",
      "An exception of type ConnectionError occurred. \n",
      "Arguments:\n",
      "(MaxRetryError(\"HTTPSConnectionPool(host='www.bing.com', port=443): Max retries exceeded with url: /search?q=animais+site%3Amedium.com+cc%3Apt-BR&qs=HS&sc=3-0&cvid=E2EAF9B741E34472B3DF8F49392E96AF&sp=1&first=9&FORM=PERE1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x0000022920508B38>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',))\",),)\n",
      "\n",
      "\n",
      " \n",
      "Blocked by Bing! Now sleeping for 600 seconds\n",
      " \n"
     ]
    }
   ],
   "source": [
    "time_list = []\n",
    "global_start_time = time.time()\n",
    "requisicao = 0\n",
    "\n",
    "        \n",
    "for j in range(idx_start, len(keywords_list)):\n",
    "    \n",
    "               \n",
    "    start_time = time.time()\n",
    "    search_term = keywords_list[j]\n",
    "    k = 1\n",
    "\n",
    "    for year in years_list:\n",
    "        \n",
    "        for k in range(9, 29, 10):\n",
    "\n",
    "\n",
    "            try:\n",
    "\n",
    "                start = str(k)\n",
    "                \n",
    "                \n",
    "                url_search_1 = 'https://www.bing.com/search?q='\n",
    "                url_search_2 = search_term + '+site%3A' + site_to_search + cc            \n",
    "                url_search_4 = '&qs=HS&sc=3-0&cvid=E2EAF9B741E34472B3DF8F49392E96AF&sp=1&first=' + str(k) + '&FORM=PERE1' \n",
    "               \n",
    "                #bing\n",
    "               \n",
    "            \n",
    "\n",
    "                # gathering the pieces of url:\n",
    "                #full_url = url_search_1 + url_search_2 + url_search_3 + url_search_4\n",
    "                full_url = url_search_1 + url_search_2 + url_search_4\n",
    "                print(full_url)\n",
    "\n",
    "                sites_list = [full_url]\n",
    "\n",
    "\n",
    "                # let's use a time sleep function before the requests in order to avoid blocks from Bing server:\n",
    "                dt = random_sleep_time()\n",
    "                print(search_term, ' ', year, 'step: ', k)\n",
    "                print('sleep time: ', dt/10)\n",
    "                time.sleep(dt/10)\n",
    "                \n",
    "                print(' ')\n",
    "                print(' ')\n",
    "\n",
    "                one_page = requests.get(str(sites_list[0]))\n",
    "                print(one_page)\n",
    "                requisicao += 1\n",
    "                \n",
    "                if one_page.status_code != 200:       \n",
    "                    print(\"ERRO\")\n",
    "\n",
    "                    filename = 'failed_keywords_google.dat'\n",
    "                    #verification folder exists\n",
    "                    if os.path.exists(str(path_results)) == True:                    \n",
    "    #                     print('The folder exists')\n",
    "\n",
    "                        if os.path.exists(str(path_results + filename)) == True:\n",
    "                            f = open(path_results + filename, 'a' )\n",
    "                        else:\n",
    "                            f = open(path_results + filename, 'w' )\n",
    "                        f.write(search_term + ',' + str(j) + ',' + str(k) + \"\\n\")\n",
    "                        f.close()  \n",
    "\n",
    "                    else:\n",
    "\n",
    "                        print('Folder does not exist')\n",
    "                        #make folder if not exists\n",
    "                        os.mkdir('results_google_standard')\n",
    "\n",
    "                        f = open(path_results + filename, 'w' )\n",
    "                        f.write(search_term + ',' + str(j) + ',' + str(k) + \"\\n\")\n",
    "                        f.close()\n",
    "                        print('Folder created')\n",
    "                    continue\n",
    "\n",
    "                    sleep_block = 600\n",
    "                    print(' ')\n",
    "                    print('Blocked by Bing! Now sleeping for {} seconds'.format(sleep_block))\n",
    "                    print(' ')\n",
    "                    time.sleep(sleep_block)\n",
    "                    continue\n",
    "\n",
    "\n",
    "                pages_set = []\n",
    "                pages_set.append(one_page)\n",
    "\n",
    "\n",
    "        # Getting the html content of all pages listed above:\n",
    "                soup_list = []\n",
    "                for ij in range(0,len(pages_set)):\n",
    "                    try:\n",
    "                        soup_list.append(BeautifulSoup(pages_set[ij].content, 'html.parser'))\n",
    "                    except:\n",
    "                        # repeat the last item in order to avoid problems.. This item is\n",
    "                        # elimanted in the last step to avoid redundance        \n",
    "                        soup_list.append(BeautifulSoup(pages_set[ij-1].content.decode('utf-8'), 'html.parser'))\n",
    "                        continue\n",
    "\n",
    "                links=[]\n",
    "                for jj in range(0,len(pages_set)):\n",
    "                    for link in soup_list[jj].find_all('a'):\n",
    "                        links.append(link.get('href'))           \n",
    "\n",
    "\n",
    "\n",
    "                linkss = []\n",
    "                for ii in range(0,len(links)):\n",
    "                    try:\n",
    "                        linkss.append(str(links[ii]))\n",
    "                        #print str(links[ii])\n",
    "                    except:\n",
    "                        continue      \n",
    "\n",
    "\n",
    "\n",
    "                #print linkss\n",
    "                #print ' '\n",
    "                #print ' '                 \n",
    "\n",
    "\n",
    "\n",
    "                archive = []\n",
    "                for item in linkss:\n",
    "                    if 'medium.com' in item and '.files.' not in item and '.bing.com' not in item:\n",
    "                        result = re.search('https://medium.com/@(.*)/', item)\n",
    "                        if result:\n",
    "                            archive.append(result.group(0))\n",
    "\n",
    "\n",
    "\n",
    "                print(archive)\n",
    "\n",
    "                archive_filtered_zero = []\n",
    "                for k in range(0,len(archive)):\n",
    "                    split_url = urlsplit(archive[k])\n",
    "                    final_url = split_url[0] + '://' + split_url[1] + split_url[2][0:-1]\n",
    "                    archive_filtered_zero.append(final_url)\n",
    "\n",
    "\n",
    "                archive_filtered_one = list(set(archive_filtered_zero))\n",
    "                print(archive_filtered_one)\n",
    "                print('collected sources: ', len(archive_filtered_one))\n",
    "\n",
    "\n",
    "\n",
    "                new_sources_list = list(set(archive_filtered_one) )    \n",
    "\n",
    "\n",
    "                #### writing the found sources to a file whose name is linked to the associated keyword.\n",
    "\n",
    "                #creating or loading the file to append the new sources found:\n",
    "                keyword_filename = 'new_links_medium_' + str(search_term) + '.dat'                \n",
    "                \n",
    "                if os.path.exists(str(path_results)) == True:                    \n",
    "#                     print('The folder exists')\n",
    "                \n",
    "                    if os.path.exists(str(path_results + keyword_filename)) == True:\n",
    "                        f = open(path_results + keyword_filename, 'a' )\n",
    "                    else:\n",
    "                        f = open(path_results + keyword_filename, 'w' )\n",
    "\n",
    "                    for k in range(0, len(new_sources_list)):\n",
    "                        f.write(new_sources_list[k]+\"\\n\")\n",
    "                    f.close()\n",
    "                    \n",
    "                else:\n",
    "                    print('Folder does not exist')\n",
    "                    os.mkdir('results_medium_standard')\n",
    "\n",
    "                    f = open(path_results + keyword_filename, 'w' )\n",
    "\n",
    "                    for k in range(0, len(new_sources_list)):\n",
    "                        f.write(new_sources_list[k]+\"\\n\")\n",
    "                    f.close()\n",
    "\n",
    "\n",
    "                # reading the file to eliminate duplicates:\n",
    "                final_list_keyword = []\n",
    "                current_folder =  str(os.getcwd()+'/')\n",
    "                f = open(str(path_results + keyword_filename), 'r')\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    final_list_keyword.append(line)\n",
    "                f.close()    \n",
    "\n",
    "\n",
    "                # writing the de-duplicated source list to a final file:\n",
    "                keyword_filename_final = 'new_links_medium_' + str(search_term) + '_filtered.dat'\n",
    "                final_list_keyword_filtered = list(set(final_list_keyword))\n",
    "                f = open(path_results + keyword_filename_final, 'w' )\n",
    "                for k in range(0, len(final_list_keyword_filtered)):\n",
    "                    f.write(final_list_keyword_filtered[k]+\"\\n\")\n",
    "                f.close()                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                ### writing the found sources to a global pool\n",
    "\n",
    "                #creating or loading the file to append the new sources found:\n",
    "                if os.path.exists(str(path_results +'new_links_medium.dat')) == True:\n",
    "                    f = open(path_results + 'new_links_medium.dat', 'a' )\n",
    "                else:\n",
    "                    f = open(path_results + 'new_links_medium.dat', 'w' )\n",
    "                for k in range(0, len(new_sources_list)):\n",
    "                    f.write(new_sources_list[k]+\"\\n\")\n",
    "                f.close()     \n",
    "\n",
    "\n",
    "                # reading the file to eliminate duplicates:\n",
    "                final_list = []\n",
    "                f = open(str(path_results + 'new_links_medium.dat'), 'r')\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    final_list.append(line)\n",
    "                f.close()    \n",
    "\n",
    "\n",
    "                # writing the de-duplicated source list to a final file:\n",
    "                final_list_filtered = list(set(final_list))\n",
    "                f = open(path_results + 'new_links_medium_filtered.dat', 'w' )\n",
    "                for k in range(0, len(final_list_filtered)):\n",
    "                    f.write(final_list_filtered[k]+\"\\n\")\n",
    "                f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                print('Lenght new source list: ', len(final_list_filtered))\n",
    "                print(' ')\n",
    "                print(' ')\n",
    "\n",
    "\n",
    "                   \n",
    "            except Exception as ex:\n",
    "                \n",
    "                #message for specification of error\n",
    "\n",
    "                template = \"An exception of type {0} occurred. \\nArguments:\\n{1!r}\"\n",
    "                print('')\n",
    "                message = template.format(type(ex).__name__, ex.args)\n",
    "                print(message)\n",
    "                print('')\n",
    "                print('')\n",
    "\n",
    "                filename = 'failed_keywords_medium.dat'\n",
    "                #verification folder exists\n",
    "                if os.path.exists(str(path_results)) == True:                    \n",
    "#                     print('The folder exists')\n",
    "                    \n",
    "                    if os.path.exists(str(path_results + filename)) == True:\n",
    "                        f = open(path_results + filename, 'a' )\n",
    "                    else:\n",
    "                        f = open(path_results + filename, 'w' )\n",
    "                    f.write(search_term + ',' + str(j) + ',' + str(k) + \"\\n\")\n",
    "                    f.close()  \n",
    "                \n",
    "                else:\n",
    "                                   \n",
    "                    print('Folder does not exist')\n",
    "                    #make folder if not exists\n",
    "                    os.mkdir('results_medium_standard')\n",
    "\n",
    "                    f = open(path_results + filename, 'w' )\n",
    "                    f.write(search_term + ',' + str(j) + ',' + str(k) + \"\\n\")\n",
    "                    f.close()\n",
    "                    print('Folder created')\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "                sleep_block = 600\n",
    "                print(' ')\n",
    "                print('Blocked by Bing! Now sleeping for {} seconds'.format(sleep_block))\n",
    "                print(' ')\n",
    "                time.sleep(sleep_block)\n",
    "                continue                    \n",
    "\n",
    "\n",
    "\n",
    "    print('writing index to file...')\n",
    "    #print j\n",
    "    f = open(path_results + 'idx_medium_forward.dat', 'w' )\n",
    "    f.write(str(j))\n",
    "    f.close()        \n",
    "\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    dt = end_time - start_time\n",
    "    time_list.append(dt)\n",
    "    total_time_forecasting = len(keywords_list)*np.mean(time_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Step elapsed time: ', dt)\n",
    "    print('Total elapsed time: ', end_time - global_start_time)    \n",
    "    print('Total time forecasting: ', total_time_forecasting)\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "        \n",
    "        \n",
    "    \n",
    "print('Total elapsed time: ', time.time() - global_start_time)    \n",
    "print('Total requests:', requisicao)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
